{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF6dZs5E6plV"
      },
      "source": [
        "### Authors\n",
        "Cathal Crowe - 21320128 \\\n",
        "Robert Flanagan - 21311625 \\\n",
        "Steven Lavelle - 21316945\n",
        "\n",
        "### Execution\n",
        "The code executes to the end without an error.\n",
        "\n",
        "### References\n",
        "https://keras.io/examples/rl/deep_q_network_breakout \\\n",
        "https://ale.farama.org/environments/breakout \\\n",
        "https://gymnasium.farama.org \\\n",
        "https://github.com/KJ-Waller/DQN-PyTorch-Breakout/blob/master/Breakout/DQN_model.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!pip install ale_py"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DK-qGNOwtxef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccbc4940-fa1b-4001-dff4-0548213afaf9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n",
            "Collecting ale_py\n",
            "  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.10/dist-packages (from ale_py) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale_py) (4.12.2)\n",
            "Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ale_py\n",
            "Successfully installed ale_py-0.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-FmayutG6plb"
      },
      "outputs": [],
      "source": [
        "# Import our dependencies\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import (\n",
        "    AtariPreprocessing,\n",
        "    FrameStackObservation,\n",
        "    RecordVideo,\n",
        ")\n",
        "import ale_py\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H2OxPoPs6plc"
      },
      "outputs": [],
      "source": [
        "# Configure our parameters\n",
        "seed = 42\n",
        "gamma = 0.99\n",
        "learning_rate_start = 0.000032 #this could be 0.000001\n",
        "learning_rate_end = 0.0001   #this could be 0.0001\n",
        "epsilon_start = 1\n",
        "epsilon_end = 0.1\n",
        "epsilon_decay = 0.002\n",
        "min_epsilon = 0.15\n",
        "batch_size = 64\n",
        "max_steps_per_episode = 1000\n",
        "max_episodes = 500\n",
        "replay_buffer_size = 1000000\n",
        "target_update_frequency = 500\n",
        "start_training_after = 20* max_episodes #will take a while before learning\n",
        "update_after_actions = 4\n",
        "video_folder = \"recorded_episodes\"\n",
        "epsilon = epsilon_start\n",
        "#change this for greater accuracy\n",
        "learning_rate = 0.00025\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCRBiXqv6pld",
        "outputId": "d5d3f9f8-d957-4e33-a471-cd10f45d74f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/recorded_episodes folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Environment setup\n",
        "os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "gym.register_envs(ale_py)\n",
        "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n",
        "env = RecordVideo(\n",
        "    env,\n",
        "    video_folder=video_folder,\n",
        "    episode_trigger=lambda x: x % 100 == 0,\n",
        ")\n",
        "env = AtariPreprocessing(env, frame_skip=1)\n",
        "env = FrameStackObservation(env, 4)\n",
        "\n",
        "env.reset(seed=seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "x8ynUxfo6ple"
      },
      "outputs": [],
      "source": [
        "# Define our model\n",
        "num_actions = 4\n",
        "input_dim = (4, 84, 84)\n",
        "output_dim = num_actions\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        channels, _, _ = input_dim\n",
        "\n",
        "        # Three convolutional layers\n",
        "        self.l1 = nn.Sequential(\n",
        "            nn.Conv2d(channels, 32, kernel_size=8, stride=4, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Fully connected layers\n",
        "        conv_output_size = self.conv_output_dim()\n",
        "        lin1_output_size = 512\n",
        "\n",
        "        # Two linear layers\n",
        "        self.l2 = nn.Sequential(\n",
        "            nn.Linear(conv_output_size, lin1_output_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(lin1_output_size, output_dim),\n",
        "        )\n",
        "\n",
        "    # Returns the output dimension of the convolutional layers\n",
        "    def conv_output_dim(self):\n",
        "        x = torch.zeros(1, *self.input_dim)\n",
        "        x = self.l1(x)\n",
        "        return int(np.prod(x.shape))\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        actions = self.l2(x)\n",
        "\n",
        "        return actions\n",
        "\n",
        "model = DQN(input_dim, output_dim).to(device)\n",
        "model_target = DQN(input_dim, output_dim).to(device)\n",
        "model_target.load_state_dict(model.state_dict())\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9D5iCfUj6plf"
      },
      "outputs": [],
      "source": [
        "# Experience replay buffers\n",
        "action_history, state_history, state_next_history = [], [], []\n",
        "rewards_history, done_history = [], []\n",
        "episode_reward_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "frame_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wcoUpDB66plf",
        "outputId": "27e3d3d2-599b-4471-9449-b504bf7ca05e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 finished with reward 0.0, epsilon 0.9000, learning_rate 0.000250\n",
            "Episode 1 finished with reward 3.0, epsilon 0.8964, learning_rate 0.000250\n",
            "running reward: 1.50 at episode 2, frame count 500\n",
            "Episode 2 finished with reward 2.0, epsilon 0.8928, learning_rate 0.000250\n",
            "Episode 3 finished with reward 0.0, epsilon 0.8893, learning_rate 0.000250\n",
            "Episode 4 finished with reward 0.0, epsilon 0.8857, learning_rate 0.000250\n",
            "Episode 5 finished with reward 1.0, epsilon 0.8822, learning_rate 0.000250\n",
            "running reward: 1.00 at episode 6, frame count 1000\n",
            "Episode 6 finished with reward 1.0, epsilon 0.8787, learning_rate 0.000250\n",
            "Episode 7 finished with reward 2.0, epsilon 0.8751, learning_rate 0.000250\n",
            "running reward: 1.12 at episode 8, frame count 1500\n",
            "Episode 8 finished with reward 1.0, epsilon 0.8717, learning_rate 0.000250\n",
            "Episode 9 finished with reward 0.0, epsilon 0.8682, learning_rate 0.000250\n",
            "Episode 10 finished with reward 1.0, epsilon 0.8647, learning_rate 0.000250\n",
            "running reward: 1.00 at episode 11, frame count 2000\n",
            "Episode 11 finished with reward 1.0, epsilon 0.8613, learning_rate 0.000250\n",
            "Episode 12 finished with reward 1.0, epsilon 0.8578, learning_rate 0.000250\n",
            "Episode 13 finished with reward 2.0, epsilon 0.8544, learning_rate 0.000250\n",
            "running reward: 1.07 at episode 14, frame count 2500\n",
            "Episode 14 finished with reward 0.0, epsilon 0.8510, learning_rate 0.000250\n",
            "Episode 15 finished with reward 0.0, epsilon 0.8476, learning_rate 0.000250\n",
            "Episode 16 finished with reward 0.0, epsilon 0.8442, learning_rate 0.000250\n",
            "running reward: 0.88 at episode 17, frame count 3000\n",
            "Episode 17 finished with reward 2.0, epsilon 0.8408, learning_rate 0.000250\n",
            "Episode 18 finished with reward 0.0, epsilon 0.8375, learning_rate 0.000250\n",
            "running reward: 0.89 at episode 19, frame count 3500\n",
            "Episode 19 finished with reward 4.0, epsilon 0.8341, learning_rate 0.000250\n",
            "Episode 20 finished with reward 3.0, epsilon 0.8308, learning_rate 0.000250\n",
            "Episode 21 finished with reward 1.0, epsilon 0.8275, learning_rate 0.000250\n",
            "running reward: 1.14 at episode 22, frame count 4000\n",
            "Episode 22 finished with reward 0.0, epsilon 0.8242, learning_rate 0.000250\n",
            "Episode 23 finished with reward 0.0, epsilon 0.8209, learning_rate 0.000250\n",
            "Episode 24 finished with reward 2.0, epsilon 0.8176, learning_rate 0.000250\n",
            "running reward: 1.08 at episode 25, frame count 4500\n",
            "Episode 25 finished with reward 2.0, epsilon 0.8144, learning_rate 0.000250\n",
            "Episode 26 finished with reward 2.0, epsilon 0.8111, learning_rate 0.000250\n",
            "running reward: 1.15 at episode 27, frame count 5000\n",
            "Episode 27 finished with reward 1.0, epsilon 0.8079, learning_rate 0.000250\n",
            "Episode 28 finished with reward 2.0, epsilon 0.8046, learning_rate 0.000250\n",
            "Episode 29 finished with reward 0.0, epsilon 0.8014, learning_rate 0.000250\n",
            "running reward: 1.13 at episode 30, frame count 5500\n",
            "Episode 30 finished with reward 4.0, epsilon 0.7982, learning_rate 0.000250\n",
            "Episode 31 finished with reward 1.0, epsilon 0.7950, learning_rate 0.000250\n",
            "running reward: 1.22 at episode 32, frame count 6000\n",
            "Episode 32 finished with reward 1.0, epsilon 0.7919, learning_rate 0.000250\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6e340239f4c3>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;31m# Optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Plot training performance\n",
        "def plot_training_performance(episode_rewards, running_rewards):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(episode_rewards, label='Episode Reward')\n",
        "    plt.plot(running_rewards, label='Running Reward')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.title('Training Performance')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Collect episode rewards and running rewards\n",
        "episode_rewards = []\n",
        "running_rewards = []\n",
        "\n",
        "# Train the model\n",
        "try:\n",
        "    while episode_count < max_episodes:\n",
        "        observation, _ = env.reset()\n",
        "        state = np.array(observation)\n",
        "        episode_reward = 0\n",
        "\n",
        "        for timestep in range(1, max_steps_per_episode):\n",
        "            frame_count += 1\n",
        "\n",
        "            # Epsilon-greedy exploration\n",
        "            if frame_count < start_training_after or epsilon > np.random.rand(1)[0]:\n",
        "                action = np.random.choice(num_actions)\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    state_tensor = (\n",
        "                        torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "                    )\n",
        "                    action_probs = model(state_tensor)\n",
        "                    action = action_probs.argmax().cpu().item()\n",
        "\n",
        "            # Environment step\n",
        "            state_next, reward, done, _, _ = env.step(action)\n",
        "            state_next = np.array(state_next)\n",
        "\n",
        "            reward = max(min(reward, 1), -1)\n",
        "            episode_reward += reward\n",
        "\n",
        "\n",
        "            # Save experiences\n",
        "            action_history.append(action)\n",
        "            state_history.append(state)\n",
        "            state_next_history.append(state_next)\n",
        "            done_history.append(done)\n",
        "            rewards_history.append(reward)\n",
        "            state = state_next\n",
        "\n",
        "            # Update network\n",
        "            if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
        "                # Sample batch\n",
        "                indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
        "\n",
        "                # Prepare batch tensors\n",
        "                state_sample = torch.tensor(\n",
        "                    np.array([state_history[i] for i in indices]), dtype=torch.float32\n",
        "                ).to(device)\n",
        "                state_next_sample = torch.tensor(\n",
        "                    np.array([state_next_history[i] for i in indices]), dtype=torch.float32\n",
        "                ).to(device)\n",
        "                rewards_sample = torch.tensor(\n",
        "                    [rewards_history[i] for i in indices], dtype=torch.float32\n",
        "                ).to(device)\n",
        "                action_sample = torch.tensor(\n",
        "                    [action_history[i] for i in indices], dtype=torch.long\n",
        "                ).to(device)\n",
        "                done_sample = torch.tensor(\n",
        "                    [float(done_history[i]) for i in indices], dtype=torch.float32\n",
        "                ).to(device)\n",
        "\n",
        "                # Predict future rewards\n",
        "                with torch.no_grad():\n",
        "                    future_rewards = model_target(state_next_sample)\n",
        "                    updated_q_values = rewards_sample + gamma * future_rewards.max(1)[0] * (\n",
        "                        1 - done_sample\n",
        "                    )\n",
        "\n",
        "                # Compute Q-values\n",
        "                q_values = model(state_sample)\n",
        "                q_action = q_values.gather(1, action_sample.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = F.smooth_l1_loss(q_action, updated_q_values)\n",
        "\n",
        "                # Optimize\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Update target network\n",
        "            if frame_count % target_update_frequency == 0:\n",
        "                model_target.load_state_dict(model.state_dict())\n",
        "                print(\n",
        "                    f\"running reward: {running_reward:.2f} at episode {episode_count}, frame count {frame_count}\"\n",
        "                )\n",
        "\n",
        "            # Trim memory\n",
        "            if len(rewards_history) > replay_buffer_size:\n",
        "                for history in [\n",
        "                    rewards_history,\n",
        "                    state_history,\n",
        "                    state_next_history,\n",
        "                    action_history,\n",
        "                    done_history,\n",
        "                ]:\n",
        "                    del history[:1]\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Update epsilon and learning rate based on the current episode\n",
        "        # epsilon = max(\n",
        "        #     epsilon_end,\n",
        "        #     epsilon_start - (epsilon_start - epsilon_end) * (episode_count / max_episodes),\n",
        "        # )\n",
        "\n",
        "        # Remove 2 might be necessary for more episodes\n",
        "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-epsilon_decay * episode_count *2)\n",
        "        # learning_rate = min(\n",
        "        #     learning_rate_end,\n",
        "        #     learning_rate_start + (learning_rate_end - learning_rate_start) * (episode_count * 1.5 / max_episodes),\n",
        "        # )\n",
        "\n",
        "        # Debugging and logging\n",
        "        print(f\"Episode {episode_count} finished with reward {episode_reward}, epsilon {epsilon:.4f}, learning_rate {learning_rate:.6f}\")\n",
        "\n",
        "        # Collect rewards for plotting\n",
        "        episode_reward_history.append(episode_reward)\n",
        "        if len(episode_reward_history) > 100:\n",
        "            del episode_reward_history[:1]\n",
        "        running_reward = np.mean(episode_reward_history)\n",
        "        running_rewards.append(running_reward)\n",
        "        episode_rewards.append(episode_reward)\n",
        "\n",
        "        episode_count += 1\n",
        "\n",
        "        if episode_count > (max_episodes * 0.3):\n",
        "          target_update_frequency = 1500\n",
        "\n",
        "        if episode_count > (max_episodes *0.7):\n",
        "          target_update_frequency = 2500\n",
        "\n",
        "        if episode_count % 100 ==0:\n",
        "          plot_training_performance(episode_rewards, running_rewards)\n",
        "\n",
        "        # Termination conditions\n",
        "        if running_reward > 40:\n",
        "            print(f\"Solved at episode {episode_count}!\")\n",
        "            break\n",
        "\n",
        "    print(f\"Training completed after {episode_count} episodes.\")\n",
        "finally:\n",
        "    env.close()\n",
        "\n",
        "# Plot the training performance\n",
        "plot_training_performance(episode_rewards, running_rewards)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}