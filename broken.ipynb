{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF6dZs5E6plV"
      },
      "source": [
        "### Authors\n",
        "Cathal Crowe - 21320128 \\\n",
        "Robert Flanagan - 21311625 \\\n",
        "Steven Lavelle - 21316945\n",
        "\n",
        "### Execution\n",
        "The code executes to the end without an error.\n",
        "\n",
        "### References\n",
        "https://keras.io/examples/rl/deep_q_network_breakout \\\n",
        "https://ale.farama.org/environments/breakout \\\n",
        "https://gymnasium.farama.org \\\n",
        "https://github.com/KJ-Waller/DQN-PyTorch-Breakout/blob/master/Breakout/DQN_model.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!pip install ale_py"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DK-qGNOwtxef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66aa1158-3efe-44a4-833c-599dbf9d459e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: ale_py in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.10/dist-packages (from ale_py) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale_py) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FmayutG6plb"
      },
      "outputs": [],
      "source": [
        "# Import our dependencies\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import (\n",
        "    AtariPreprocessing,\n",
        "    FrameStackObservation,\n",
        "    RecordVideo,\n",
        ")\n",
        "import ale_py\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2OxPoPs6plc"
      },
      "outputs": [],
      "source": [
        "# Configure our parameters\n",
        "seed = 42\n",
        "gamma = 0.99\n",
        "learning_rate_start = 0.0001 #this could be 0.000001\n",
        "learning_rate_end = 0.002   #this could be 0.0001\n",
        "epsilon_start = 1\n",
        "epsilon_end = 0.1\n",
        "epsilon_decay = 0.0005\n",
        "min_epsilon = 0.3\n",
        "batch_size = 64\n",
        "max_steps_per_episode = 1000\n",
        "max_episodes = 2000\n",
        "replay_buffer_size = 1000000\n",
        "target_update_frequency = 500\n",
        "start_training_after = 20* max_episodes #will take a while before learning\n",
        "update_after_actions = 4\n",
        "video_folder = \"recorded_episodes\"\n",
        "epsilon = epsilon_start\n",
        "learning_rate = learning_rate_start\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCRBiXqv6pld",
        "outputId": "5b764a46-5348-46f4-a4a8-4ea7b314fe88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Environment setup\n",
        "os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "gym.register_envs(ale_py)\n",
        "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n",
        "env = RecordVideo(\n",
        "    env,\n",
        "    video_folder=video_folder,\n",
        "    episode_trigger=lambda x: x % 100 == 0,\n",
        ")\n",
        "env = AtariPreprocessing(env, frame_skip=1)\n",
        "env = FrameStackObservation(env, 4)\n",
        "\n",
        "env.reset(seed=seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8ynUxfo6ple"
      },
      "outputs": [],
      "source": [
        "# Define our model\n",
        "num_actions = 4\n",
        "input_dim = (4, 84, 84)\n",
        "output_dim = num_actions\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        channels, _, _ = input_dim\n",
        "\n",
        "        # Three convolutional layers\n",
        "        self.l1 = nn.Sequential(\n",
        "            nn.Conv2d(channels, 32, kernel_size=8, stride=4, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Fully connected layers\n",
        "        conv_output_size = self.conv_output_dim()\n",
        "        lin1_output_size = 512\n",
        "\n",
        "        # Two linear layers\n",
        "        self.l2 = nn.Sequential(\n",
        "            nn.Linear(conv_output_size, lin1_output_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(lin1_output_size, output_dim),\n",
        "        )\n",
        "\n",
        "    # Returns the output dimension of the convolutional layers\n",
        "    def conv_output_dim(self):\n",
        "        x = torch.zeros(1, *self.input_dim)\n",
        "        x = self.l1(x)\n",
        "        return int(np.prod(x.shape))\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        actions = self.l2(x)\n",
        "\n",
        "        return actions\n",
        "\n",
        "model = DQN(input_dim, output_dim).to(device)\n",
        "model_target = DQN(input_dim, output_dim).to(device)\n",
        "model_target.load_state_dict(model.state_dict())\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D5iCfUj6plf"
      },
      "outputs": [],
      "source": [
        "# Experience replay buffers\n",
        "action_history, state_history, state_next_history = [], [], []\n",
        "rewards_history, done_history = [], []\n",
        "episode_reward_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "frame_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wcoUpDB66plf",
        "outputId": "519e1343-d0fa-426c-fdee-13780fc8bd79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 finished with reward 1.0, epsilon 1.0000, learning_rate 0.000100\n",
            "Episode 1 finished with reward 0.0, epsilon 0.9995, learning_rate 0.000104\n",
            "running reward: 0.50 at episode 2, frame count 500\n",
            "Episode 2 finished with reward 1.0, epsilon 0.9990, learning_rate 0.000108\n",
            "Episode 3 finished with reward 3.0, epsilon 0.9985, learning_rate 0.000111\n",
            "running reward: 1.25 at episode 4, frame count 1000\n",
            "Episode 4 finished with reward 2.0, epsilon 0.9980, learning_rate 0.000115\n",
            "Episode 5 finished with reward 2.0, epsilon 0.9975, learning_rate 0.000119\n",
            "Episode 6 finished with reward 1.0, epsilon 0.9970, learning_rate 0.000123\n",
            "running reward: 1.43 at episode 7, frame count 1500\n",
            "Episode 7 finished with reward 2.0, epsilon 0.9965, learning_rate 0.000127\n",
            "Episode 8 finished with reward 0.0, epsilon 0.9960, learning_rate 0.000130\n",
            "Episode 9 finished with reward 2.0, epsilon 0.9956, learning_rate 0.000134\n",
            "running reward: 1.40 at episode 10, frame count 2000\n",
            "Episode 10 finished with reward 2.0, epsilon 0.9951, learning_rate 0.000138\n",
            "Episode 11 finished with reward 1.0, epsilon 0.9946, learning_rate 0.000142\n",
            "Episode 12 finished with reward 0.0, epsilon 0.9941, learning_rate 0.000146\n",
            "running reward: 1.31 at episode 13, frame count 2500\n",
            "Episode 13 finished with reward 2.0, epsilon 0.9936, learning_rate 0.000149\n",
            "Episode 14 finished with reward 2.0, epsilon 0.9931, learning_rate 0.000153\n",
            "running reward: 1.40 at episode 15, frame count 3000\n",
            "Episode 15 finished with reward 2.0, epsilon 0.9926, learning_rate 0.000157\n",
            "Episode 16 finished with reward 0.0, epsilon 0.9921, learning_rate 0.000161\n",
            "Episode 17 finished with reward 1.0, epsilon 0.9916, learning_rate 0.000165\n",
            "running reward: 1.33 at episode 18, frame count 3500\n",
            "Episode 18 finished with reward 1.0, epsilon 0.9911, learning_rate 0.000168\n",
            "Episode 19 finished with reward 2.0, epsilon 0.9906, learning_rate 0.000172\n",
            "Episode 20 finished with reward 0.0, epsilon 0.9901, learning_rate 0.000176\n",
            "running reward: 1.29 at episode 21, frame count 4000\n",
            "Episode 21 finished with reward 1.0, epsilon 0.9897, learning_rate 0.000180\n",
            "Episode 22 finished with reward 1.0, epsilon 0.9892, learning_rate 0.000184\n",
            "Episode 23 finished with reward 0.0, epsilon 0.9887, learning_rate 0.000187\n",
            "running reward: 1.21 at episode 24, frame count 4500\n",
            "Episode 24 finished with reward 3.0, epsilon 0.9882, learning_rate 0.000191\n",
            "Episode 25 finished with reward 3.0, epsilon 0.9877, learning_rate 0.000195\n",
            "running reward: 1.35 at episode 26, frame count 5000\n",
            "Episode 26 finished with reward 0.0, epsilon 0.9872, learning_rate 0.000199\n",
            "Episode 27 finished with reward 4.0, epsilon 0.9867, learning_rate 0.000203\n",
            "running reward: 1.39 at episode 28, frame count 5500\n",
            "Episode 28 finished with reward 4.0, epsilon 0.9862, learning_rate 0.000206\n",
            "Episode 29 finished with reward 1.0, epsilon 0.9857, learning_rate 0.000210\n",
            "running reward: 1.47 at episode 30, frame count 6000\n",
            "Episode 30 finished with reward 4.0, epsilon 0.9853, learning_rate 0.000214\n",
            "Episode 31 finished with reward 2.0, epsilon 0.9848, learning_rate 0.000218\n",
            "running reward: 1.56 at episode 32, frame count 6500\n",
            "Episode 32 finished with reward 5.0, epsilon 0.9843, learning_rate 0.000222\n",
            "Episode 33 finished with reward 1.0, epsilon 0.9838, learning_rate 0.000225\n",
            "Episode 34 finished with reward 2.0, epsilon 0.9833, learning_rate 0.000229\n",
            "running reward: 1.66 at episode 35, frame count 7000\n",
            "Episode 35 finished with reward 0.0, epsilon 0.9828, learning_rate 0.000233\n",
            "Episode 36 finished with reward 2.0, epsilon 0.9823, learning_rate 0.000237\n",
            "running reward: 1.62 at episode 37, frame count 7500\n",
            "Episode 37 finished with reward 1.0, epsilon 0.9819, learning_rate 0.000241\n",
            "Episode 38 finished with reward 0.0, epsilon 0.9814, learning_rate 0.000244\n",
            "Episode 39 finished with reward 0.0, epsilon 0.9809, learning_rate 0.000248\n",
            "Episode 40 finished with reward 0.0, epsilon 0.9804, learning_rate 0.000252\n",
            "running reward: 1.49 at episode 41, frame count 8000\n",
            "Episode 41 finished with reward 0.0, epsilon 0.9799, learning_rate 0.000256\n",
            "Episode 42 finished with reward 1.0, epsilon 0.9794, learning_rate 0.000260\n",
            "Episode 43 finished with reward 2.0, epsilon 0.9789, learning_rate 0.000263\n",
            "running reward: 1.45 at episode 44, frame count 8500\n",
            "Episode 44 finished with reward 3.0, epsilon 0.9785, learning_rate 0.000267\n",
            "Episode 45 finished with reward 2.0, epsilon 0.9780, learning_rate 0.000271\n",
            "running reward: 1.50 at episode 46, frame count 9000\n",
            "Episode 46 finished with reward 1.0, epsilon 0.9775, learning_rate 0.000275\n",
            "Episode 47 finished with reward 1.0, epsilon 0.9770, learning_rate 0.000279\n",
            "Episode 48 finished with reward 0.0, epsilon 0.9765, learning_rate 0.000282\n",
            "running reward: 1.45 at episode 49, frame count 9500\n",
            "Episode 49 finished with reward 0.0, epsilon 0.9760, learning_rate 0.000286\n",
            "Episode 50 finished with reward 2.0, epsilon 0.9756, learning_rate 0.000290\n",
            "Episode 51 finished with reward 2.0, epsilon 0.9751, learning_rate 0.000294\n",
            "running reward: 1.44 at episode 52, frame count 10000\n",
            "Episode 52 finished with reward 4.0, epsilon 0.9746, learning_rate 0.000298\n",
            "Episode 53 finished with reward 0.0, epsilon 0.9741, learning_rate 0.000301\n",
            "running reward: 1.46 at episode 54, frame count 10500\n",
            "Episode 54 finished with reward 1.0, epsilon 0.9736, learning_rate 0.000305\n",
            "Episode 55 finished with reward 3.0, epsilon 0.9731, learning_rate 0.000309\n",
            "running reward: 1.48 at episode 56, frame count 11000\n",
            "Episode 56 finished with reward 4.0, epsilon 0.9727, learning_rate 0.000313\n",
            "Episode 57 finished with reward 3.0, epsilon 0.9722, learning_rate 0.000317\n",
            "Episode 58 finished with reward 0.0, epsilon 0.9717, learning_rate 0.000320\n",
            "running reward: 1.53 at episode 59, frame count 11500\n",
            "Episode 59 finished with reward 1.0, epsilon 0.9712, learning_rate 0.000324\n",
            "Episode 60 finished with reward 1.0, epsilon 0.9707, learning_rate 0.000328\n",
            "Episode 61 finished with reward 0.0, epsilon 0.9703, learning_rate 0.000332\n",
            "running reward: 1.48 at episode 62, frame count 12000\n",
            "Episode 62 finished with reward 1.0, epsilon 0.9698, learning_rate 0.000336\n",
            "running reward: 1.48 at episode 63, frame count 12500\n",
            "Episode 63 finished with reward 5.0, epsilon 0.9693, learning_rate 0.000339\n",
            "Episode 64 finished with reward 0.0, epsilon 0.9688, learning_rate 0.000343\n",
            "Episode 65 finished with reward 0.0, epsilon 0.9683, learning_rate 0.000347\n",
            "running reward: 1.48 at episode 66, frame count 13000\n",
            "Episode 66 finished with reward 3.0, epsilon 0.9679, learning_rate 0.000351\n",
            "Episode 67 finished with reward 3.0, epsilon 0.9674, learning_rate 0.000355\n",
            "Episode 68 finished with reward 0.0, epsilon 0.9669, learning_rate 0.000358\n",
            "running reward: 1.51 at episode 69, frame count 13500\n",
            "Episode 69 finished with reward 2.0, epsilon 0.9664, learning_rate 0.000362\n",
            "Episode 70 finished with reward 2.0, epsilon 0.9659, learning_rate 0.000366\n",
            "running reward: 1.52 at episode 71, frame count 14000\n",
            "Episode 71 finished with reward 3.0, epsilon 0.9655, learning_rate 0.000370\n",
            "Episode 72 finished with reward 3.0, epsilon 0.9650, learning_rate 0.000374\n",
            "running reward: 1.56 at episode 73, frame count 14500\n",
            "Episode 73 finished with reward 0.0, epsilon 0.9645, learning_rate 0.000377\n",
            "Episode 74 finished with reward 0.0, epsilon 0.9640, learning_rate 0.000381\n",
            "Episode 75 finished with reward 0.0, epsilon 0.9636, learning_rate 0.000385\n",
            "running reward: 1.50 at episode 76, frame count 15000\n",
            "Episode 76 finished with reward 3.0, epsilon 0.9631, learning_rate 0.000389\n",
            "Episode 77 finished with reward 1.0, epsilon 0.9626, learning_rate 0.000393\n",
            "Episode 78 finished with reward 1.0, epsilon 0.9621, learning_rate 0.000396\n",
            "running reward: 1.51 at episode 79, frame count 15500\n",
            "Episode 79 finished with reward 2.0, epsilon 0.9617, learning_rate 0.000400\n",
            "Episode 80 finished with reward 1.0, epsilon 0.9612, learning_rate 0.000404\n",
            "Episode 81 finished with reward 0.0, epsilon 0.9607, learning_rate 0.000408\n",
            "running reward: 1.49 at episode 82, frame count 16000\n",
            "Episode 82 finished with reward 1.0, epsilon 0.9602, learning_rate 0.000412\n",
            "Episode 83 finished with reward 0.0, epsilon 0.9598, learning_rate 0.000415\n",
            "Episode 84 finished with reward 0.0, epsilon 0.9593, learning_rate 0.000419\n",
            "running reward: 1.45 at episode 85, frame count 16500\n",
            "Episode 85 finished with reward 2.0, epsilon 0.9588, learning_rate 0.000423\n",
            "Episode 86 finished with reward 0.0, epsilon 0.9583, learning_rate 0.000427\n",
            "Episode 87 finished with reward 1.0, epsilon 0.9579, learning_rate 0.000431\n",
            "running reward: 1.43 at episode 88, frame count 17000\n",
            "Episode 88 finished with reward 0.0, epsilon 0.9574, learning_rate 0.000434\n",
            "Episode 89 finished with reward 0.0, epsilon 0.9569, learning_rate 0.000438\n",
            "Episode 90 finished with reward 1.0, epsilon 0.9564, learning_rate 0.000442\n",
            "running reward: 1.40 at episode 91, frame count 17500\n",
            "Episode 91 finished with reward 1.0, epsilon 0.9560, learning_rate 0.000446\n",
            "Episode 92 finished with reward 0.0, epsilon 0.9555, learning_rate 0.000450\n",
            "Episode 93 finished with reward 2.0, epsilon 0.9550, learning_rate 0.000453\n",
            "running reward: 1.38 at episode 94, frame count 18000\n",
            "Episode 94 finished with reward 1.0, epsilon 0.9545, learning_rate 0.000457\n",
            "Episode 95 finished with reward 0.0, epsilon 0.9541, learning_rate 0.000461\n",
            "Episode 96 finished with reward 1.0, epsilon 0.9536, learning_rate 0.000465\n",
            "running reward: 1.36 at episode 97, frame count 18500\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-162-efa31f44d339>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# Environment step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mstate_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mstate_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/stateful_observation.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mStacked\u001b[0m \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \"\"\"\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/atari_preprocessing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/rendering.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    348\u001b[0m     ) -> tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001b[1;32m    349\u001b[0m         \u001b[0;34m\"\"\"Steps through the environment using action, recording observations if :attr:`self.recording`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    320\u001b[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001b[1;32m    321\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ale_py/env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mis_terminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_truncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Plot training performance\n",
        "def plot_training_performance(episode_rewards, running_rewards):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(episode_rewards, label='Episode Reward')\n",
        "    plt.plot(running_rewards, label='Running Reward')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.title('Training Performance')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Collect episode rewards and running rewards\n",
        "episode_rewards = []\n",
        "running_rewards = []\n",
        "\n",
        "# Train the model\n",
        "try:\n",
        "    while episode_count < max_episodes:\n",
        "        observation, _ = env.reset()\n",
        "        state = np.array(observation)\n",
        "        episode_reward = 0\n",
        "\n",
        "        for timestep in range(1, max_steps_per_episode):\n",
        "            frame_count += 1\n",
        "\n",
        "            # Epsilon-greedy exploration\n",
        "            if frame_count < start_training_after or epsilon > np.random.rand(1)[0]:\n",
        "                action = np.random.choice(num_actions)\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    state_tensor = (\n",
        "                        torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "                    )\n",
        "                    action_probs = model(state_tensor)\n",
        "                    action = action_probs.argmax().cpu().item()\n",
        "\n",
        "            # Environment step\n",
        "            state_next, reward, done, _, _ = env.step(action)\n",
        "            state_next = np.array(state_next)\n",
        "\n",
        "            reward = max(min(reward, 1), -1)\n",
        "            episode_reward += reward\n",
        "\n",
        "            if episode_count > 750:\n",
        "              target_update_frequency = 1000\n",
        "\n",
        "            # Save experiences\n",
        "            action_history.append(action)\n",
        "            state_history.append(state)\n",
        "            state_next_history.append(state_next)\n",
        "            done_history.append(done)\n",
        "            rewards_history.append(reward)\n",
        "            state = state_next\n",
        "\n",
        "            # Update network\n",
        "            if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
        "                # Sample batch\n",
        "                indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
        "\n",
        "                # Prepare batch tensors\n",
        "                state_sample = torch.tensor(\n",
        "                    np.array([state_history[i] for i in indices]), dtype=torch.float32\n",
        "                ).to(device)\n",
        "                state_next_sample = torch.tensor(\n",
        "                    np.array([state_next_history[i] for i in indices]), dtype=torch.float32\n",
        "                ).to(device)\n",
        "                rewards_sample = torch.tensor(\n",
        "                    [rewards_history[i] for i in indices], dtype=torch.float32\n",
        "                ).to(device)\n",
        "                action_sample = torch.tensor(\n",
        "                    [action_history[i] for i in indices], dtype=torch.long\n",
        "                ).to(device)\n",
        "                done_sample = torch.tensor(\n",
        "                    [float(done_history[i]) for i in indices], dtype=torch.float32\n",
        "                ).to(device)\n",
        "\n",
        "                # Predict future rewards\n",
        "                with torch.no_grad():\n",
        "                    future_rewards = model_target(state_next_sample)\n",
        "                    updated_q_values = rewards_sample + gamma * future_rewards.max(1)[0] * (\n",
        "                        1 - done_sample\n",
        "                    )\n",
        "\n",
        "                # Compute Q-values\n",
        "                q_values = model(state_sample)\n",
        "                q_action = q_values.gather(1, action_sample.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = F.smooth_l1_loss(q_action, updated_q_values)\n",
        "\n",
        "                # Optimize\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Update target network\n",
        "            if frame_count % target_update_frequency == 0:\n",
        "                model_target.load_state_dict(model.state_dict())\n",
        "                print(\n",
        "                    f\"running reward: {running_reward:.2f} at episode {episode_count}, frame count {frame_count}\"\n",
        "                )\n",
        "\n",
        "            # Trim memory\n",
        "            if len(rewards_history) > replay_buffer_size:\n",
        "                for history in [\n",
        "                    rewards_history,\n",
        "                    state_history,\n",
        "                    state_next_history,\n",
        "                    action_history,\n",
        "                    done_history,\n",
        "                ]:\n",
        "                    del history[:1]\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Update epsilon and learning rate based on the current episode\n",
        "        # epsilon = max(\n",
        "        #     epsilon_end,\n",
        "        #     epsilon_start - (epsilon_start - epsilon_end) * (episode_count / max_episodes),\n",
        "        # )\n",
        "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-epsilon_decay * episode_count)\n",
        "        learning_rate = min(\n",
        "            learning_rate_end,\n",
        "            learning_rate_start + (learning_rate_end - learning_rate_start) * (episode_count / max_episodes),\n",
        "        )\n",
        "\n",
        "        # Debugging and logging\n",
        "        print(f\"Episode {episode_count} finished with reward {episode_reward}, epsilon {epsilon:.4f}, learning_rate {learning_rate:.6f}\")\n",
        "\n",
        "        # Collect rewards for plotting\n",
        "        episode_reward_history.append(episode_reward)\n",
        "        if len(episode_reward_history) > 100:\n",
        "            del episode_reward_history[:1]\n",
        "        running_reward = np.mean(episode_reward_history)\n",
        "        running_rewards.append(running_reward)\n",
        "\n",
        "        episode_count += 1\n",
        "\n",
        "        # Termination conditions\n",
        "        if running_reward > 40:\n",
        "            print(f\"Solved at episode {episode_count}!\")\n",
        "            break\n",
        "\n",
        "    print(f\"Training completed after {episode_count} episodes.\")\n",
        "finally:\n",
        "    env.close()\n",
        "\n",
        "# Plot the training performance\n",
        "plot_training_performance(episode_rewards, running_rewards)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}