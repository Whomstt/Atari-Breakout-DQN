{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors\n",
    "Cathal Crowe - 21320128 \\\n",
    "Robert Flanagan - 21311625 \\\n",
    "Steven Lavelle - 21316945\n",
    "\n",
    "### Execution\n",
    "The code executes to the end without an error.\n",
    "\n",
    "### References\n",
    "https://keras.io/examples/rl/deep_q_network_breakout \\\n",
    "https://ale.farama.org/environments/breakout \\\n",
    "https://gymnasium.farama.org \\\n",
    "https://github.com/KJ-Waller/DQN-PyTorch-Breakout/blob/master/Breakout/DQN_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import (\n",
    "    AtariPreprocessing,\n",
    "    FrameStackObservation,\n",
    "    RecordVideo,\n",
    ")\n",
    "import ale_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure our parameters\n",
    "seed = 42\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 1.0\n",
    "epsilon_interval = epsilon_max - epsilon_min\n",
    "batch_size = 32\n",
    "max_steps_per_episode = 10000\n",
    "max_episodes = 2000\n",
    "video_folder = \"recorded_episodes\"\n",
    "epsilon_random_frames = 50000\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "max_memory_length = 100000\n",
    "update_after_actions = 4\n",
    "update_target_network = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\catha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\catha\\Documents\\CollegeProjects\\CS4287Assign3\\recorded_episodes folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "os.makedirs(video_folder, exist_ok=True)\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=video_folder,\n",
    "    episode_trigger=lambda x: x % 10 == 0,\n",
    ")\n",
    "env = AtariPreprocessing(env, frame_skip=1)\n",
    "env = FrameStackObservation(env, 4)\n",
    "\n",
    "env.reset(seed=seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our model\n",
    "num_actions = 4\n",
    "input_dim = (4, 84, 84)\n",
    "output_dim = num_actions\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        channels, _, _ = input_dim\n",
    "\n",
    "        # Three convolutional layers\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, kernel_size=8, stride=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        conv_output_size = self.conv_output_dim()\n",
    "        lin1_output_size = 512\n",
    "\n",
    "        # Two linear layers\n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Linear(conv_output_size, lin1_output_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(lin1_output_size, output_dim),\n",
    "        )\n",
    "\n",
    "    # Returns the output dimension of the convolutional layers\n",
    "    def conv_output_dim(self):\n",
    "        x = torch.zeros(1, *self.input_dim)\n",
    "        x = self.l1(x)\n",
    "        return int(np.prod(x.shape))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        actions = self.l2(x)\n",
    "\n",
    "        return actions\n",
    "\n",
    "model = DQN(input_dim, output_dim).to(device)\n",
    "model_target = DQN(input_dim, output_dim).to(device)\n",
    "model_target.load_state_dict(model.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay buffers\n",
    "action_history, state_history, state_next_history = [], [], []\n",
    "rewards_history, done_history = [], []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with reward 0.0\n",
      "Episode 2 finished with reward 2.0\n",
      "Episode 3 finished with reward 0.0\n",
      "Episode 4 finished with reward 0.0\n",
      "Episode 5 finished with reward 1.0\n",
      "Episode 6 finished with reward 2.0\n",
      "Episode 7 finished with reward 1.0\n",
      "Episode 8 finished with reward 2.0\n",
      "Episode 9 finished with reward 0.0\n",
      "Episode 10 finished with reward 2.0\n",
      "Episode 11 finished with reward 2.0\n",
      "Episode 12 finished with reward 1.0\n",
      "Episode 13 finished with reward 1.0\n",
      "Episode 14 finished with reward 1.0\n",
      "Episode 15 finished with reward 0.0\n",
      "Episode 16 finished with reward 0.0\n",
      "Episode 17 finished with reward 1.0\n",
      "Episode 18 finished with reward 1.0\n",
      "Episode 19 finished with reward 3.0\n",
      "Episode 20 finished with reward 0.0\n",
      "Episode 21 finished with reward 0.0\n",
      "Episode 22 finished with reward 2.0\n",
      "Episode 23 finished with reward 0.0\n",
      "Episode 24 finished with reward 0.0\n",
      "Episode 25 finished with reward 3.0\n",
      "Episode 26 finished with reward 1.0\n",
      "Episode 27 finished with reward 2.0\n",
      "Episode 28 finished with reward 0.0\n",
      "Episode 29 finished with reward 3.0\n",
      "Episode 30 finished with reward 0.0\n",
      "Episode 31 finished with reward 0.0\n",
      "Episode 32 finished with reward 2.0\n",
      "Episode 33 finished with reward 0.0\n",
      "Episode 34 finished with reward 1.0\n",
      "Episode 35 finished with reward 0.0\n",
      "Episode 36 finished with reward 3.0\n",
      "Episode 37 finished with reward 1.0\n",
      "Episode 38 finished with reward 4.0\n",
      "Episode 39 finished with reward 3.0\n",
      "Episode 40 finished with reward 4.0\n",
      "Episode 41 finished with reward 1.0\n",
      "Episode 42 finished with reward 2.0\n",
      "Episode 43 finished with reward 0.0\n",
      "Episode 44 finished with reward 0.0\n",
      "Episode 45 finished with reward 1.0\n",
      "Episode 46 finished with reward 1.0\n",
      "Episode 47 finished with reward 0.0\n",
      "Episode 48 finished with reward 1.0\n",
      "Episode 49 finished with reward 0.0\n",
      "Episode 50 finished with reward 0.0\n",
      "Episode 51 finished with reward 2.0\n",
      "Episode 52 finished with reward 1.0\n",
      "Episode 53 finished with reward 1.0\n",
      "Episode 54 finished with reward 1.0\n",
      "Episode 55 finished with reward 0.0\n",
      "Episode 56 finished with reward 2.0\n",
      "running reward: 1.11 at episode 56, frame count 10000\n",
      "Episode 57 finished with reward 0.0\n",
      "Episode 58 finished with reward 4.0\n",
      "Episode 59 finished with reward 2.0\n",
      "Episode 60 finished with reward 0.0\n",
      "Episode 61 finished with reward 1.0\n",
      "Episode 62 finished with reward 2.0\n",
      "Episode 63 finished with reward 3.0\n",
      "Episode 64 finished with reward 2.0\n",
      "Episode 65 finished with reward 1.0\n",
      "Episode 66 finished with reward 2.0\n",
      "Episode 67 finished with reward 1.0\n",
      "Episode 68 finished with reward 1.0\n",
      "Episode 69 finished with reward 0.0\n",
      "Episode 70 finished with reward 2.0\n",
      "Episode 71 finished with reward 2.0\n",
      "Episode 72 finished with reward 0.0\n",
      "Episode 73 finished with reward 3.0\n",
      "Episode 74 finished with reward 2.0\n",
      "Episode 75 finished with reward 1.0\n",
      "Episode 76 finished with reward 3.0\n",
      "Episode 77 finished with reward 2.0\n",
      "Episode 78 finished with reward 1.0\n",
      "Episode 79 finished with reward 1.0\n",
      "Episode 80 finished with reward 0.0\n",
      "Episode 81 finished with reward 1.0\n",
      "Episode 82 finished with reward 1.0\n",
      "Episode 83 finished with reward 1.0\n",
      "Episode 84 finished with reward 0.0\n",
      "Episode 85 finished with reward 2.0\n",
      "Episode 86 finished with reward 1.0\n",
      "Episode 87 finished with reward 0.0\n",
      "Episode 88 finished with reward 1.0\n",
      "Episode 89 finished with reward 3.0\n",
      "Episode 90 finished with reward 2.0\n",
      "Episode 91 finished with reward 2.0\n",
      "Episode 92 finished with reward 6.0\n",
      "Episode 93 finished with reward 1.0\n",
      "Episode 94 finished with reward 1.0\n",
      "Episode 95 finished with reward 2.0\n",
      "Episode 96 finished with reward 1.0\n",
      "Episode 97 finished with reward 0.0\n",
      "Episode 98 finished with reward 0.0\n",
      "Episode 99 finished with reward 3.0\n",
      "Episode 100 finished with reward 3.0\n",
      "Episode 101 finished with reward 0.0\n",
      "Episode 102 finished with reward 1.0\n",
      "Episode 103 finished with reward 0.0\n",
      "Episode 104 finished with reward 0.0\n",
      "Episode 105 finished with reward 0.0\n",
      "Episode 106 finished with reward 0.0\n",
      "Episode 107 finished with reward 1.0\n",
      "Episode 108 finished with reward 0.0\n",
      "Episode 109 finished with reward 1.0\n",
      "Episode 110 finished with reward 0.0\n",
      "running reward: 1.22 at episode 110, frame count 20000\n",
      "Episode 111 finished with reward 4.0\n",
      "Episode 112 finished with reward 0.0\n",
      "Episode 113 finished with reward 0.0\n",
      "Episode 114 finished with reward 0.0\n",
      "Episode 115 finished with reward 1.0\n",
      "Episode 116 finished with reward 0.0\n",
      "Episode 117 finished with reward 0.0\n",
      "Episode 118 finished with reward 3.0\n",
      "Episode 119 finished with reward 0.0\n",
      "Episode 120 finished with reward 3.0\n",
      "Episode 121 finished with reward 1.0\n",
      "Episode 122 finished with reward 1.0\n",
      "Episode 123 finished with reward 0.0\n",
      "Episode 124 finished with reward 0.0\n",
      "Episode 125 finished with reward 1.0\n",
      "Episode 126 finished with reward 3.0\n",
      "Episode 127 finished with reward 2.0\n",
      "Episode 128 finished with reward 1.0\n",
      "Episode 129 finished with reward 3.0\n",
      "Episode 130 finished with reward 1.0\n",
      "Episode 131 finished with reward 0.0\n",
      "Episode 132 finished with reward 2.0\n",
      "Episode 133 finished with reward 0.0\n",
      "Episode 134 finished with reward 2.0\n",
      "Episode 135 finished with reward 2.0\n",
      "Episode 136 finished with reward 0.0\n",
      "Episode 137 finished with reward 0.0\n",
      "Episode 138 finished with reward 0.0\n",
      "Episode 139 finished with reward 1.0\n",
      "Episode 140 finished with reward 3.0\n",
      "Episode 141 finished with reward 0.0\n",
      "Episode 142 finished with reward 1.0\n",
      "Episode 143 finished with reward 2.0\n",
      "Episode 144 finished with reward 1.0\n",
      "Episode 145 finished with reward 1.0\n",
      "Episode 146 finished with reward 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(epsilon, epsilon_min)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Environment step\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m state_next, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m state_next \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(state_next)\n\u001b[0;32m     29\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mc:\\Users\\catha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\stateful_observation.py:416\u001b[0m, in \u001b[0;36mFrameStackObservation.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    407\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    408\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \n\u001b[0;32m    410\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;124;03m        Stacked observations, reward, terminated, truncated, and info from the environment\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 416\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_queue\u001b[38;5;241m.\u001b[39mappend(obs)\n\u001b[0;32m    419\u001b[0m     updated_obs \u001b[38;5;241m=\u001b[39m deepcopy(\n\u001b[0;32m    420\u001b[0m         concatenate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mobservation_space, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_queue, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstacked_obs)\n\u001b[0;32m    421\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\catha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\atari_preprocessing.py:184\u001b[0m, in \u001b[0;36mAtariPreprocessing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgetScreenRGB(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_buffer[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, total_reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\catha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\atari_preprocessing.py:219\u001b[0m, in \u001b[0;36mAtariPreprocessing._get_obs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    215\u001b[0m     np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_buffer[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_buffer[\u001b[38;5;241m1\u001b[39m], out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_buffer[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINTER_AREA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_obs:\n\u001b[0;32m    226\u001b[0m     obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(obs, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "try:\n",
    "    while True:\n",
    "        observation, _ = env.reset()\n",
    "        state = np.array(observation)\n",
    "        episode_reward = 0\n",
    "\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            frame_count += 1\n",
    "\n",
    "            # Epsilon-greedy exploration\n",
    "            if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "                action = np.random.choice(num_actions)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = (\n",
    "                        torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                    )\n",
    "                    action_probs = model(state_tensor)\n",
    "                    action = action_probs.argmax().cpu().item()\n",
    "\n",
    "            # Decay exploration\n",
    "            epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "            epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "            # Environment step\n",
    "            state_next, reward, done, _, _ = env.step(action)\n",
    "            state_next = np.array(state_next)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Save experiences\n",
    "            action_history.append(action)\n",
    "            state_history.append(state)\n",
    "            state_next_history.append(state_next)\n",
    "            done_history.append(done)\n",
    "            rewards_history.append(reward)\n",
    "            state = state_next\n",
    "\n",
    "            # Update network\n",
    "            if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "                # Sample batch\n",
    "                indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "                # Prepare batch tensors\n",
    "                state_sample = torch.tensor(\n",
    "                    np.array([state_history[i] for i in indices]), dtype=torch.float32\n",
    "                ).to(device)\n",
    "                state_next_sample = torch.tensor(\n",
    "                    np.array([state_next_history[i] for i in indices]), dtype=torch.float32\n",
    "                ).to(device)\n",
    "                rewards_sample = torch.tensor(\n",
    "                    [rewards_history[i] for i in indices], dtype=torch.float32\n",
    "                ).to(device)\n",
    "                action_sample = torch.tensor(\n",
    "                    [action_history[i] for i in indices], dtype=torch.long\n",
    "                ).to(device)\n",
    "                done_sample = torch.tensor(\n",
    "                    [float(done_history[i]) for i in indices], dtype=torch.float32\n",
    "                ).to(device)\n",
    "\n",
    "                # Predict future rewards\n",
    "                with torch.no_grad():\n",
    "                    future_rewards = model_target(state_next_sample)\n",
    "                    updated_q_values = rewards_sample + gamma * future_rewards.max(1)[0] * (\n",
    "                        1 - done_sample\n",
    "                    )\n",
    "\n",
    "                # Compute Q-values\n",
    "                q_values = model(state_sample)\n",
    "                q_action = q_values.gather(1, action_sample.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = F.smooth_l1_loss(q_action, updated_q_values)\n",
    "\n",
    "                # Optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Update target network\n",
    "            if frame_count % update_target_network == 0:\n",
    "                model_target.load_state_dict(model.state_dict())\n",
    "                print(\n",
    "                    f\"running reward: {running_reward:.2f} at episode {episode_count}, frame count {frame_count}\"\n",
    "                )\n",
    "\n",
    "            # Trim memory\n",
    "            if len(rewards_history) > max_memory_length:\n",
    "                for history in [\n",
    "                    rewards_history,\n",
    "                    state_history,\n",
    "                    state_next_history,\n",
    "                    action_history,\n",
    "                    done_history,\n",
    "                ]:\n",
    "                    del history[:1]\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Update running reward\n",
    "        episode_reward_history.append(episode_reward)\n",
    "        if len(episode_reward_history) > 100:\n",
    "            del episode_reward_history[:1]\n",
    "        running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "        episode_count += 1\n",
    "\n",
    "        print(f\"Episode {episode_count} finished with reward {episode_reward}\")\n",
    "\n",
    "        # Termination conditions\n",
    "        if running_reward > 40:\n",
    "            print(f\"Solved at episode {episode_count}!\")\n",
    "            break\n",
    "\n",
    "        if max_episodes > 0 and episode_count >= max_episodes:\n",
    "            print(f\"Stopped at episode {episode_count}!\")\n",
    "            break\n",
    "finally:\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
