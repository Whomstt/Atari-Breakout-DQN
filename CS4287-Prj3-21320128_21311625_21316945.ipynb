{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors\n",
    "Cathal Crowe - 21320128 \\\n",
    "Robert Flanagan - 21311625 \\\n",
    "Steven Lavelle - 21316945\n",
    "\n",
    "### Execution\n",
    "The code executes to the end without an error.\n",
    "\n",
    "### References\n",
    "https://keras.io/examples/rl/deep_q_network_breakout - Adapted network structure and hyperparameters\n",
    "https://github.com/KJ-Waller/DQN-PyTorch-Breakout/blob/master/Breakout/DQN_model.py - Adapted network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import (\n",
    "    AtariPreprocessing,\n",
    "    FrameStackObservation,\n",
    "    RecordVideo,\n",
    ")\n",
    "import ale_py\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure our parameters\n",
    "seed = 42\n",
    "gamma = 0.99\n",
    "learning_rate = 0.0001\n",
    "max_episodes = 1500\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = np.exp(np.log(epsilon_min / epsilon_max) / max_episodes)\n",
    "weight_decay = 0.0001\n",
    "batch_size = 64\n",
    "max_steps_per_episode = 1000\n",
    "replay_buffer_size = 100000\n",
    "target_update_frequency = 1000\n",
    "start_training_after = 1000\n",
    "update_after_actions = 4\n",
    "video_folder = \"recorded_episodes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track highest reward and episode\n",
    "highest_reward = 0\n",
    "highest_reward_episode = 0\n",
    "\n",
    "# Environment setup\n",
    "os.makedirs(video_folder, exist_ok=True)\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=video_folder,\n",
    "    episode_trigger=lambda x: x % 100 == 0,\n",
    ")\n",
    "env = AtariPreprocessing(env, frame_skip=1)\n",
    "env = FrameStackObservation(env, 4)\n",
    "\n",
    "env.reset(seed=seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our model\n",
    "num_actions = 3\n",
    "input_dim = (4, 84, 84)\n",
    "output_dim = num_actions\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        channels, _, _ = input_dim\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, kernel_size=8, stride=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Compute the output size of the convolutional layers\n",
    "        conv_output_size = self.conv_output_dim()\n",
    "        hidden_size = 512\n",
    "\n",
    "        # Separate stream for values\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(conv_output_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),  # Outputs scalar state-value V(s)\n",
    "        )\n",
    "\n",
    "        # Separate stream for advantages\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(conv_output_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_dim),  # Outputs advantages A(s, a)\n",
    "        )\n",
    "\n",
    "    # Compute the output size of the convolutional layers\n",
    "    def conv_output_dim(self):\n",
    "        x = torch.zeros(1, *self.input_dim)\n",
    "        x = self.feature_layer(x)\n",
    "        return int(np.prod(x.shape))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.feature_layer(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Compute value and advantages\n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "\n",
    "        # Combine streams to calculate Q-values\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "\n",
    "# Initialize our model and target model\n",
    "model = DuelingDQN(input_dim, output_dim).to(device)\n",
    "model_target = DuelingDQN(input_dim, output_dim).to(device)\n",
    "model_target.load_state_dict(model.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay buffers\n",
    "action_history, state_history, state_next_history = [], [], []\n",
    "rewards_history, done_history = [], []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training performance\n",
    "def plot_training_performance(episode_rewards, running_rewards):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(episode_rewards, label='Episode Reward')\n",
    "    plt.plot(running_rewards, label='Running Reward')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Training Performance')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Plot epsilon decay\n",
    "def plot_epsilon_decay(epsilon_values):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(epsilon_values, label=\"Epsilon\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Epsilon Value\")\n",
    "    plt.title(\"Epsilon Decay Over Episodes\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect episode rewards, running rewards, and epsilon values across episodes\n",
    "episode_rewards = []\n",
    "running_rewards = []\n",
    "epsilon_values = []\n",
    "\n",
    "# Set epsilon to 1 for exploration\n",
    "epsilon = epsilon_max\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    while True:\n",
    "        observation, _ = env.reset()\n",
    "        state = np.array(observation)\n",
    "        episode_reward = 0\n",
    "\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            frame_count += 1\n",
    "\n",
    "            # Epsilon-greedy exploration\n",
    "            if frame_count < start_training_after or np.random.rand(1)[0] < epsilon:\n",
    "                action = np.random.choice(num_actions)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = (\n",
    "                        torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                    )\n",
    "                    action_probs = model(state_tensor)\n",
    "                    action = action_probs.argmax().cpu().item()\n",
    "\n",
    "            # Environment step\n",
    "            state_next, reward, done, _, _ = env.step(action)\n",
    "            state_next = np.array(state_next)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Save experiences\n",
    "            action_history.append(action)\n",
    "            state_history.append(state)\n",
    "            state_next_history.append(state_next)\n",
    "            done_history.append(done)\n",
    "            rewards_history.append(reward)\n",
    "            state = state_next\n",
    "\n",
    "            # Update network\n",
    "            if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "                # Sample batch\n",
    "                indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "                # Prepare batch tensors\n",
    "                state_sample = torch.tensor(\n",
    "                    np.array([state_history[i] for i in indices]), dtype=torch.float32\n",
    "                ).to(device)\n",
    "                state_next_sample = torch.tensor(\n",
    "                    np.array([state_next_history[i] for i in indices]), dtype=torch.float32\n",
    "                ).to(device)\n",
    "                rewards_sample = torch.tensor(\n",
    "                    [rewards_history[i] for i in indices], dtype=torch.float32\n",
    "                ).to(device)\n",
    "                action_sample = torch.tensor(\n",
    "                    [action_history[i] for i in indices], dtype=torch.long\n",
    "                ).to(device)\n",
    "                done_sample = torch.tensor(\n",
    "                    [float(done_history[i]) for i in indices], dtype=torch.float32\n",
    "                ).to(device)\n",
    "\n",
    "                # Double DQN logic - select action using the main network\n",
    "                with torch.no_grad():\n",
    "                    # Get actions from the main network\n",
    "                    action_next = model(state_next_sample).argmax(1)\n",
    "                    # Evaluate actions using the target network\n",
    "                    target_q_values = (\n",
    "                        model_target(state_next_sample)\n",
    "                        .gather(1, action_next.unsqueeze(1))\n",
    "                        .squeeze(1)\n",
    "                    )\n",
    "                    updated_q_values = rewards_sample + gamma * target_q_values * (\n",
    "                        1 - done_sample\n",
    "                    )\n",
    "\n",
    "                # Compute Q-values\n",
    "                q_values = model(state_sample)\n",
    "                q_action = q_values.gather(1, action_sample.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = F.smooth_l1_loss(q_action, updated_q_values)\n",
    "\n",
    "                # Optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Update target network\n",
    "            if frame_count % target_update_frequency == 0:\n",
    "                model_target.load_state_dict(model.state_dict())\n",
    "\n",
    "            # Trim memory\n",
    "            if len(rewards_history) > replay_buffer_size:\n",
    "                for history in [\n",
    "                    rewards_history,\n",
    "                    state_history,\n",
    "                    state_next_history,\n",
    "                    action_history,\n",
    "                    done_history,\n",
    "                ]:\n",
    "                    del history[:1]\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        # Decay exploration (epsilon) after each episode\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        episode_count += 1\n",
    "\n",
    "        # Update highest reward and episode\n",
    "        if episode_reward > highest_reward:\n",
    "            highest_reward = episode_reward\n",
    "            highest_reward_episode = episode_count\n",
    "    \n",
    "        # Collect rewards for plotting\n",
    "        episode_rewards.append(episode_reward)\n",
    "        running_reward = np.mean(episode_rewards[-100:])\n",
    "        running_rewards.append(running_reward)\n",
    "\n",
    "        # Collect epsilon values for plotting\n",
    "        epsilon_values.append(epsilon)\n",
    "            \n",
    "        print(\n",
    "            f\"Episode {episode_count} - Reward: {episode_reward:.3f}, \"\n",
    "            f\"Running Reward: {running_reward:.3f}, Epsilon: {epsilon:.3f}, \"\n",
    "            f\"Highest Reward: {highest_reward:.3f} (Episode {highest_reward_episode})\"\n",
    "        )\n",
    "\n",
    "        # Termination condition\n",
    "        if episode_count >= max_episodes:\n",
    "            print(f\"Stopped at episode {episode_count}!\")\n",
    "            # Save the trained model\n",
    "            torch.save(model.state_dict(), \"dqn_trained_model.pth\")\n",
    "            print(\"Model saved to dqn_trained_model.pth\")\n",
    "            break\n",
    "finally:\n",
    "    env.close()\n",
    "\n",
    "# Plot the training performance\n",
    "plot_training_performance(episode_rewards, running_rewards)\n",
    "\n",
    "# Plot the epsilon decay\n",
    "plot_epsilon_decay(epsilon_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Reinforcement Learning is the machine learning paradigm of choice for this task\n",
    "The goal of this project is to train a machine learning paradigm to learn to play an Atari game. The game we have chosen is Breakout due to its simplicity and easily trackable reward. Reinforcement learning is a type of machine learning where the agent learns to make choices by interacting with its environment with the goal of maximizing its reward. There are a number of reasons why reinforcement learning is the most suitable choice. Reinforcement learning's ability to learn through interaction, optimize rewards and handle sequential decisions make it the obvious choice for this task.\n",
    "## Sequential Decision Making\n",
    "Breakout is a game comprised of sequential decisions where at every frame the agent must decide to move left move right or stay central. Each decision changes the game state and effects the next decision. Due to this decisions need to be made with long term strategy in mind instead of just focusing on short term reward. This makes reinforcement learning the ideal choice as it focuses on maximising long-term reward.\n",
    "## Deep Learning\n",
    "Reinforcement learning can leverage deep learning techniques in the form of a DQN. This allows it to handle the high dimensional input of a game like breakout.\n",
    "## Exploration and Exploitation\n",
    "To maximise reward, it is important not to converge on sub optimal strategies. Reinforcement learning utilises epsilon-greedy exploration to ensure there is a balance between exploiting (choosing the action the agent knows will get it more reward immediately) and exploring (making a decision that improves the agent's knowledge and might lead to better long-term reward).\n",
    "## Unlabelled Data\n",
    "Machine learning paradigms such as reinforcement learning that rely on labelled data would be unsuitable for this task as games like breakout have no explicit labels mapping actions to rewards. This means the chosen paradigm must discover strategy through trial and error which is exactly how reinforcement learning works.\n",
    "# The Gym Environment\n",
    "Gymnasium is a project that provides an API for all single agent reinforcement learning environments. For this project we use the \"ALE/Breakout-v5\" gym environment provided by The Arcade Learning Environment (ALE) which provides a simulation of Breakout allowing our agent to interact with and learn from the game through an interface. It outputs images that represent the game state which includes the important elements like the ball, the paddle and the bricks. The agent analyses these images to determine its actions. The agent has 3 actions to choose from on each frame these being moving left, moving right or staying still. The reward system is central to how our agent learns. In this game the agent earns rewards by breaking blocks and keeping the ball in play. By interacting with the environment, the agent learns the optimal strategies to achieve the highest reward.\n",
    "# Implementation\n",
    "## Capture and pre-processing of the data\n",
    "### Pre-processing\n",
    "Our data is pre-processed using the AtariPreprocessing wrapper imported from Gymnasium. All parameters are left on the default ones except that we specify our frame_skip parameter to 1, as we already skip 4 frames based on the environment’s default internal frame skipping with gym.make(\"ALE/Breakout-v5\"), leaving this field blank would be stacking frame skipping leading to undesirable results. The screen size of the processed screen is set to 84px. We convert our frames to grayscale to reduce complexity as colour is of no interest to us. This reduces the input dimensionality into a single channel as opposed to 3 with RGB. Overall, our pre-processing is setup to reduce the complexity of our input, while preserving essential information for learning. This provides us with more efficient data to train off.\n",
    "### Capture\n",
    "We store episode rewards and running rewards in lists. Episode rewards are a list of all rewards earned from each episode. Running rewards are a list of all rewards earned over the last 100 episodes and are used to provide an average for the current state of the model. The environment is reset at the start of each episode, the variable episode_reward tracks the total reward for the current episode. If exploration is enabled (determined by epsilon-greedy strategy), the agent randomly chooses an action. Otherwise, the agent uses the model to select the action with highest value by exploitation. After taking an action, the environment returns the next state, reward, and flag stating whether the episode has finished. The agent stores experiences in lists, and once enough experiences are obtained, the agent samples a random batch of experiences for training. We do this to ensure that our model learns from diverse experiences. We then apply our double DQN update by computing target Q-values based on the target network’s predictions, which are then compared with the Q-values from the main network to calculate the loss. The network updates are then updated using backpropagation along with the Adam optimizer. The target network’s weights are updated periodically to match the main network. This allows us to stabilize training by using a fixed target for the Q-value updates. After each episode the epsilon decays to reduce the amount of exploration over time. We record every 100 episodes as an mp4 video file to view our model in action and to be able to observe the results.\n",
    "## The Network Structure \n",
    "Our network structure is built is accordance with a Dueling DQN architecture. Our network is built to handle an input of 4 frames of 84x84 pixels stacked on each other. Are output dimensions are set as the number of actions in Atari Breakout, which is 3 (move left, move right, don’t move). The number of channels is set to 4, one for each frame.\n",
    "```python\n",
    "num_actions = 3\n",
    "input_dim = (4, 84, 84)\n",
    "output_dim = num_actions\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        channels, _, _ = input_dim\n",
    "```\n",
    "For feature extraction, we have a sequence of 3 convolutional layers to extract spatial features in the input images. The first layer takes our 4 input channels and applies 32 filters with a kernel size of 8x8, a stride of 4 and a padding of 2. The second layer applies 64 filters with a kernel size of 4x4, a stride of 2 and a padding of 1. Finally, the third layer applies 64 filters with a kernel size 3, a stride of 1 and a padding of 1. We apply the ReLU activation  function after each convolution to help the network learn complex patterns. After our input passes through these layers, we are left with our feature map.\n",
    "```python\n",
    "self.feature_layer = nn.Sequential(\n",
    "    nn.Conv2d(channels, 32, kernel_size=8, stride=4, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "```\n",
    "We have two separate streams in our network. The value stream is used to estimate the value of the state, which is the expected value returned by that state. To achieve this we use a fully connected layer. The first linear layer maps the output of our convolutional layers to a hidden layer of size 512. We then apply a ReLU activation function to introduce non-linearity. The second layer reduces this to a scalar output representing the state value.\n",
    "```python\n",
    "conv_output_size = self.conv_output_dim()\n",
    "hidden_size = 512\n",
    "\n",
    "self.value_stream = nn.Sequential(\n",
    "    nn.Linear(conv_output_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, 1),\n",
    ")\n",
    "```\n",
    "The advantage stream is used to estimate the advantage or relative benefit of each action in a given state. This is very similar to our value stream, however, instead of outputting a singular scalar value, we output a vector of size 3, where each element in this vector corresponds to each action.\n",
    "```python\n",
    "self.advantage_stream = nn.Sequential(\n",
    "    nn.Linear(conv_output_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, output_dim),\n",
    ")\n",
    "```\n",
    "The forward pass passes our input first through the feature layer to extract spatial features as stated previously. We then flatten the output to a 1D vector to make it suitable for our fully connected layers. We then compute the value and advantage using our streams. The final q value is then calculated by combining the value and advantage. This is the final Q-value for each action in a given state.\n",
    "```python\n",
    "def forward(self, x):\n",
    "    x = self.feature_layer(x)\n",
    "    x = x.view(x.shape[0], -1)\n",
    "    value = self.value_stream(x)\n",
    "    advantage = self.advantage_stream(x)\n",
    "    q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "    return q_values\n",
    "```\n",
    "Our conv_output_dim function computes the output size of our convolutional layers. As we are decreasing the spatial dimensions of the input after each layer, we use this function to calculate the size of the feature map after passing through all convolutional layers.\n",
    "```python\n",
    "def conv_output_dim(self):\n",
    "    x = torch.zeros(1, *self.input_dim)\n",
    "    x = self.feature_layer(x)\n",
    "    return int(np.prod(x.shape))\n",
    "```\n",
    "We initialize both our models of the Dueling DQN to support our Double DQN setup. We use the main network to select the next action, and the model target to evaluate the action selected by the main network. We then compute the updated Q-value based on this evaluation and change the Q-value the updated one. We also initialize our Adam optimizer here with our learning rate and weight decay variables.\n",
    "```python\n",
    "model = DuelingDQN(input_dim, output_dim).to(device)\n",
    "model_target = DuelingDQN(input_dim, output_dim).to(device)\n",
    "model_target.load_state_dict(model.state_dict())\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "```\n",
    "## Hyperparameters\n",
    "We used various hyperparameters throughout creating this DQN for Atari Breakout. The hyperparameters had a large effect on the output of rewards and how successful the DQN was. We changed the hyperparameters multiple times with varying success, either increasing running rewards or significantly decreasing running rewards, causing our DQN to struggle to get a score above 4 in some cases.\n",
    "```python\n",
    "gamma = 0.99\n",
    "learning_rate = 0.0001\n",
    "max_episodes = 1500\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = np.exp(np.log(epsilon_min / epsilon_max) / max_episodes)\n",
    "weight_decay = 0.0001\n",
    "batch_size = 64\n",
    "max_steps_per_episode = 1000\n",
    "replay_buffer_size = 100000\n",
    "target_update_frequency = 1000\n",
    "start_training_after = 1000\n",
    "update_after_actions = 4\n",
    "```\n",
    "### Gamma\n",
    "Gamma was set to 0.99, this is to maximize future rewards as the higher the score obtained the greater rewards received.\n",
    "### Learning Rate\n",
    "We found that a learning rate of 0.0001 was sufficient as this allowed us to accurately narrow down on consistent running rewards. We used various other learning rates like 0.00025 and larger learning rates, but they gave us diminished rewards in comparison, converging earlier. We also tried a decreasing and increasing learning rate but a constant learning rate worked best with the decaying epsilon, when fine tuning. \n",
    "### Number of Episodes\n",
    "We ran 1500 episodes, this gave us a convergence, we only ran 1500 episodes due to computational limitations. After 1500 episodes we got reduced rewards, with little overall change in episode rewards.\n",
    "### Epsilon\n",
    "We use a varied epsilon value that decayed exponentially over time, we start off running an epsilon value of 1 initially and this decays quickly in the beginning then slowing down to the minimum value of 0.01. This means the DQN will only make a random move 1% of the time, which means not much learning occurs here, and the DQN is more or less trained, as good as it is going to get. Our epsilon decays to 0.01 over the number of episodes ran so if we increase the number of episodes the decay is slower.\n",
    "### Weight Decay\n",
    "We use a weight decay value of 0.0001 through the Adam optimizer. This value appears to be optimal in nudging our weights toward zero to prevent them from growing excessively large. This value allowed us to stabilize our training at no additional overhead.\n",
    "### Batch Size\n",
    "We use a batch size of 64, this reduces variance in episode rewards in comparison with 32 that we used prior, and we also looked at 128 which didn’t have a noticeable difference, other than increasing computation needed. 64 provides a nice balance in providing previous experiences for training, while being computationally feasible.\n",
    "### Max steps per episode\n",
    "The max number of steps that can taking per episode is set at 5000, this allowed the DQN to take a variety of actions, without it getting stuck in a loop without further reward. This allows for experimentation and increases learning.\n",
    "### Replay Buffer Size\n",
    "The replay buffer size was set to 100,000 this lets the DQN recall on past experiences, replacing old ones with newer ones once the buffer size has been reached. We had our DQN update every 1000 frames, this allowed us to vary the weights constantly while also training the DQN. If the target update frequency was increased by too much, it takes longer to converge overall running rewards.\n",
    "### Target Update Frequency\n",
    "We update our target network after every 1000 steps, as it balances stability and responsiveness. When updated too frequently, the target network became too similar to the main network, reducing stability. Infrequent updates caused the target network to become too outdated, leading to inaccurate Q-value targets\n",
    "### Start Training After\n",
    "We do not start training initially until after 1000 steps, allowing the replay buffer to have some information stored. The network then updates after the first 1000 steps occur, so until then random exploration takes place. This is quite slow as we train our model very early as epsilon is decaying over time and dictates our random movements.\n",
    "### Update after actions\n",
    "Updates occur after every 4 actions using batches of 64 that we mentioned before. This stops the network from making constant updates letting the network learn.\n",
    "## Where the Q learning update is applied to the weights\n",
    "The Q-learning rate is updated on each episode iteration to find the best Q-value possible over the training of our agent. When the while loop begins the environment is reset and episode reward returns to 0. Then a for loop keeps track of timestep to run outlined steps per episode, ensuring the agent doesn’t get stuck in a loop making no progress. The agent doesn’t update Q values initially until after start training is exceeded or a random number is less than epsilon which will more than likely be true for as long as epsilon is still over 0.5. If both are false, the agent will go with exploitation over exploration and the model predicts the action with the highest Q value at this current state.\n",
    "The else part disables gradient computation reducing overall computation and memory. The state is converted to a PyTorch tensor and unsqueeze helps convert in the games input dimensions. The action is then selected based on which has the highest predicted Q value.\n",
    "```python\n",
    "try:\n",
    "    while True:\n",
    "        observation, _ = env.reset()\n",
    "        state = np.array(observation)\n",
    "        episode_reward = 0\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            frame_count += 1\n",
    "            if frame_count < start_training_after or np.random.rand(1)[0] < epsilon:\n",
    "                action = np.random.choice(num_actions)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = (\n",
    "                        torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                    )\n",
    "                    action_probs = model(state_tensor)\n",
    "                    action = action_probs.argmax().cpu().item()\n",
    "```\n",
    "We store experiences containing information on state, action, reward, next state, done in a replay buffer. This allows our model to sample experiences from the replay buffer when training.\n",
    "```python\n",
    "action_history.append(action)\n",
    "state_history.append(state)\n",
    "state_next_history.append(state_next)\n",
    "done_history.append(done)\n",
    "rewards_history.append(reward)\n",
    "state = state_next\n",
    "```\n",
    "The gradient computation is disabled so the code inside doesn’t affect gradient, and a batch of samples are taking from the replay buffer we have. The Q values for the next states are got and it selects the one with the highest Q value. The target then calculates the Q value for the chosen action. The Q values are correlated to the corresponding actions. The Q value is then updated combining rewards observed and future rewards multiplied by target Q values and discounting future rewards when episode is done. Basically, the model selects the best actions and the target checks those actions and ensures the Q values are stable. The Q values are then added together with rewards which forms the Bellman target.\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    action_next = model(state_next_sample).argmax(1)\n",
    "    target_q_values = (\n",
    "        model_target(state_next_sample)\n",
    "        .gather(1, action_next.unsqueeze(1))\n",
    "        .squeeze(1)\n",
    "    )\n",
    "    updated_q_values = rewards_sample + gamma * target_q_values * (\n",
    "        1 - done_sample\n",
    "    )\n",
    "```\n",
    "After an action is selected the Q-values are calculated by passing state_sample through the model. The model then outputs the Q-values which represent the expected future rewards of each potential action in the current state. We use the gather function in order to retrieve the Q-value of the current action which is stored as q_action. Next, we calculate the loss by calculating the difference between the predicated Q-values and the updated Q-values with the F_smooth_l1_loss function.  How this function works is it punishes small errors heavily using a squared error to help the agent make small adjustments for better accuracy. However, for large errors it takes the absolute error to reduce the impact of outliers. This loss value shows how far the models' predictions are from the true value. Next, we update the model by first resetting the optimizer with optimzer.zero.grad() which clears the gradients from the previous steps. Then we call loss.backward() to compute the new gradient based on the current loss. Finally, optimizer.step() updates the models parameters by applying the calculated gradients\n",
    "```python\n",
    "q_values = model(state_sample)\n",
    "q_action = q_values.gather(1, action_sample.unsqueeze(1)).squeeze(1)\n",
    "loss = F.smooth_l1_loss(q_action, updated_q_values)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "## Independently researched concepts\n",
    "### Random Seed Initialization\n",
    "We use random seed initialization to ensure reproducibility in our models training and performance. To achieve this, we set a variable seed = 42 and set the gym environment accordingly. By setting the seed to a constant value, we can achieve consistent behaviour across our runs, allowing us to tune our hyperparameters effectively without considering the degree of variance in different seeds. Without using a seed, our results would vary due to the stochastic nature of the model training process, this would make it very difficult for us to evaluate the model’s performance.\n",
    "```python\n",
    "env.reset(seed=seed)\n",
    "```\n",
    "### Impact of Regularizers on Scores\n",
    "We use regularization techniques to reduce the overfitting of the model and improve generalization. In our code, we implement L2 regularization through the weight_decay parameter passed to our Adam optimizer. The purpose of this is to penalize large weights during training by the addition of a term proportional to the sum of squared weights to the loss function, this effectively constrains our model’s complexity and improves stability. L2 regularization also encourages simplicity in our model to ensure a smoother convergence.\n",
    "```python\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "```\n",
    "### Techniques to counter Catastrophic Forgetting\n",
    "We implemented techniques to counter catastrophic forgetting (the tendency of neural networks to forget / lose previously learned information when trained on new data). To accomplish this, we created an experience replay to store a buffer of past experiences that we can sample from throughout the training process. This method prevents the agent from solely focusing on recent experiences. We also have a second network (the target network) that updates less frequently, and we use this to calculate more stable Q-values.\n",
    "```python\n",
    "action_history, state_history, state_next_history = [], [], []\n",
    "rewards_history, done_history = [], []\n",
    "if frame_count % target_update_frequency == 0:\n",
    "    model_target.load_state_dict(model.state_dict())\n",
    "```\n",
    "### Techniques to counter Maximization Bias (Double DQN)\n",
    "Maximization Bias arises in Q-Learning when we use the same network for both action selection and Q-value estimates. This approach can often overestimate Q-values. To counter this problem, we implemented a double q-learning implementation that resolves this bias by separating the action selection and Q-value estimation. In other words, the main network selects our action, and the target network evaluates it, reducing the overestimation.\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    action_next = model(state_next_sample).argmax(1)\n",
    "    target_q_values = (\n",
    "        model_target(state_next_sample)\n",
    "        .gather(1, action_next.unsqueeze(1))\n",
    "        .squeeze(1)\n",
    "    )\n",
    "    updated_q_values = rewards_sample + gamma * target_q_values * (\n",
    "        1 - done_sample\n",
    "    )\n",
    "```\n",
    "### Dueling DQN\n",
    "A Dueling DQN is an enhanced DQN used in reinforced learning, the dueling DQN is better since it has increased stability and performance. This type of DQN uses two streams instead of the usual single stream to calculate the value function and advantage function. The two streams are then combined to create the final Q-values for each action an agent may take. Dueling DQNs allow for more efficient learning and overall achieve better results than basic single stream DQNs. In our implementation of the DQN model, we implemented a Dueling DQN. This gives our network the ability to differentiate between the quality of a state and the specific benefits attached to each action.\n",
    "```python\n",
    "self.value_stream = nn.Sequential(\n",
    "    nn.Linear(conv_output_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, 1),\n",
    ")\n",
    "self.advantage_stream = nn.Sequential(\n",
    "    nn.Linear(conv_output_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, output_dim),\n",
    ")\n",
    "```\n",
    "### Loss\n",
    "We made use of Smooth L1 Loss to stabilize the training. It does this in making our training more resilient to outliers and noisy transitions, thus “smoothing” out our data. This loss function is widely used in reinforcement learning, especially in Deep Q-Networks because it balances stability and adaptability.\n",
    "```python\n",
    "loss = F.smooth_l1_loss(q_action, updated_q_values)\n",
    "```\n",
    "# Plots\n",
    "# Videos\n",
    "# Evaluation of the results\n",
    "Overall, our DQN achieves a reasonable average running reward of 16, frequently gets scores over 25 and on occasion can get a score of over 30 in later episodes. We are happy enough with this result as there are so many hyperparameters that can be varied and many attempts received little to no increase in reward. We originally were stuck on an average running reward of 4 or 5 due to some attempts yielding little to no reward. We believe a higher score is achievable with an increase in episodes ran and small changes to the hyperparameters to accommodate for a slower learning rate. Our DQN quickly trains once beginning training after 1000 steps. This is due to learning rate improving constantly and epsilon (which effects randomness) becoming lower over time. Running reward then slows after reaching 500 episodes before picking up and climbing to a running reward of 16 and plateauing, indicating little change in policy. Plateauing occurs due to epsilon reducing to 0.01 which means the DQN only makes random moves 1% of the time. It is clear from initial episodes and videos that the agent figures out the ball drops to the far left upon start.\n",
    "\n",
    "# References\n",
    "https://keras.io/examples/rl/deep_q_network_breakout - Adapted network structure and hyperparameters<br>\n",
    "https://github.com/KJ-Waller/DQN-PyTorch-Breakout/blob/master/Breakout/DQN_model.py - Adapted network structure<br>\n",
    "https://gymnasium.farama.org<br>\n",
    "https://ale.farama.org<br>\n",
    "https://pytorch.org/docs<br>\n",
    "Mnih, V., et al. 2013. Playing Atari with Deep Reinforcement Learning<br>\n",
    "Mousavi, S.S., Schukat, M. and Howley, E. 2018. Deep Reinforcement Learning: An Overview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
