{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors\n",
    "Cathal Crowe - 21320128 \\\n",
    "Robert Flanagan - 21311625 \\\n",
    "Steven Lavelle - 21316945\n",
    "\n",
    "### Execution\n",
    "The code executes to the end without an error.\n",
    "\n",
    "### References\n",
    "https://keras.io/examples/rl/deep_q_network_breakout \\\n",
    "https://ale.farama.org/environments/breakout \\\n",
    "https://gymnasium.farama.org \\\n",
    "https://github.com/KJ-Waller/DQN-PyTorch-Breakout/blob/master/Breakout/DQN_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import (\n",
    "    AtariPreprocessing,\n",
    "    FrameStackObservation,\n",
    "    RecordVideo,\n",
    ")\n",
    "import ale_py\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure our parameters\n",
    "seed = 42\n",
    "gamma = 0.99\n",
    "learning_rate = 0.0001\n",
    "max_episodes = 1500\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = np.exp(np.log(epsilon_min / epsilon_max) / max_episodes)\n",
    "weight_decay = 0.0001\n",
    "batch_size = 64\n",
    "max_steps_per_episode = 1000\n",
    "replay_buffer_size = 100000\n",
    "target_update_frequency = 1000\n",
    "start_training_after = 1000\n",
    "update_after_actions = 4\n",
    "video_folder = \"recorded_episodes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track highest reward and episode\n",
    "highest_reward = 0\n",
    "highest_reward_episode = 0\n",
    "\n",
    "# Environment setup\n",
    "os.makedirs(video_folder, exist_ok=True)\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=video_folder,\n",
    "    episode_trigger=lambda x: x % 100 == 0,\n",
    ")\n",
    "env = AtariPreprocessing(env, frame_skip=1)\n",
    "env = FrameStackObservation(env, 4)\n",
    "\n",
    "env.reset(seed=seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our model\n",
    "num_actions = 3\n",
    "input_dim = (4, 84, 84)\n",
    "output_dim = num_actions\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        channels, _, _ = input_dim\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, kernel_size=8, stride=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Compute the output size of the convolutional layers\n",
    "        conv_output_size = self.conv_output_dim()\n",
    "        hidden_size = 512\n",
    "\n",
    "        # Separate stream for values\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(conv_output_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),  # Outputs scalar state-value V(s)\n",
    "        )\n",
    "\n",
    "        # Separate stream for advantages\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(conv_output_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_dim),  # Outputs advantages A(s, a)\n",
    "        )\n",
    "\n",
    "    # Compute the output size of the convolutional layers\n",
    "    def conv_output_dim(self):\n",
    "        x = torch.zeros(1, *self.input_dim)\n",
    "        x = self.feature_layer(x)\n",
    "        return int(np.prod(x.shape))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.feature_layer(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Compute value and advantages\n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "\n",
    "        # Combine streams to calculate Q-values\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "\n",
    "# Initialize our model and target model\n",
    "model = DuelingDQN(input_dim, output_dim).to(device)\n",
    "model_target = DuelingDQN(input_dim, output_dim).to(device)\n",
    "model_target.load_state_dict(model.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay buffers\n",
    "action_history, state_history, state_next_history = [], [], []\n",
    "rewards_history, done_history = [], []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training performance\n",
    "def plot_training_performance(episode_rewards, running_rewards):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(episode_rewards, label='Episode Reward')\n",
    "    plt.plot(running_rewards, label='Running Reward')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Training Performance')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect episode rewards and running rewards\n",
    "episode_rewards = []\n",
    "running_rewards = []\n",
    "\n",
    "# Set epsilon to 1 for exploration\n",
    "epsilon = epsilon_max\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    while True:\n",
    "        observation, _ = env.reset()\n",
    "        state = np.array(observation)\n",
    "        episode_reward = 0\n",
    "\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            frame_count += 1\n",
    "\n",
    "            # Epsilon-greedy exploration\n",
    "            if frame_count < start_training_after or np.random.rand(1)[0] < epsilon:\n",
    "                action = np.random.choice(num_actions)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = (\n",
    "                        torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                    )\n",
    "                    action_probs = model(state_tensor)\n",
    "                    action = action_probs.argmax().cpu().item()\n",
    "\n",
    "            # Environment step\n",
    "            state_next, reward, done, _, _ = env.step(action)\n",
    "            state_next = np.array(state_next)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Save experiences\n",
    "            action_history.append(action)\n",
    "            state_history.append(state)\n",
    "            state_next_history.append(state_next)\n",
    "            done_history.append(done)\n",
    "            rewards_history.append(reward)\n",
    "            state = state_next\n",
    "\n",
    "            # Update network\n",
    "            if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "                # Sample batch\n",
    "                indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "                # Prepare batch tensors\n",
    "                state_sample = torch.tensor(\n",
    "                    np.array([state_history[i] for i in indices]), dtype=torch.float32\n",
    "                ).to(device)\n",
    "                state_next_sample = torch.tensor(\n",
    "                    np.array([state_next_history[i] for i in indices]), dtype=torch.float32\n",
    "                ).to(device)\n",
    "                rewards_sample = torch.tensor(\n",
    "                    [rewards_history[i] for i in indices], dtype=torch.float32\n",
    "                ).to(device)\n",
    "                action_sample = torch.tensor(\n",
    "                    [action_history[i] for i in indices], dtype=torch.long\n",
    "                ).to(device)\n",
    "                done_sample = torch.tensor(\n",
    "                    [float(done_history[i]) for i in indices], dtype=torch.float32\n",
    "                ).to(device)\n",
    "\n",
    "                # Double DQN logic - select action using the main network\n",
    "                with torch.no_grad():\n",
    "                    # Get actions from the main network\n",
    "                    action_next = model(state_next_sample).argmax(1)\n",
    "                    # Evaluate actions using the target network\n",
    "                    target_q_values = (\n",
    "                        model_target(state_next_sample)\n",
    "                        .gather(1, action_next.unsqueeze(1))\n",
    "                        .squeeze(1)\n",
    "                    )\n",
    "                    updated_q_values = rewards_sample + gamma * target_q_values * (\n",
    "                        1 - done_sample\n",
    "                    )\n",
    "\n",
    "                # Compute Q-values\n",
    "                q_values = model(state_sample)\n",
    "                q_action = q_values.gather(1, action_sample.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = F.smooth_l1_loss(q_action, updated_q_values)\n",
    "\n",
    "                # Optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Update target network\n",
    "            if frame_count % target_update_frequency == 0:\n",
    "                model_target.load_state_dict(model.state_dict())\n",
    "\n",
    "            # Trim memory\n",
    "            if len(rewards_history) > replay_buffer_size:\n",
    "                for history in [\n",
    "                    rewards_history,\n",
    "                    state_history,\n",
    "                    state_next_history,\n",
    "                    action_history,\n",
    "                    done_history,\n",
    "                ]:\n",
    "                    del history[:1]\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        # Decay exploration (epsilon) after each episode\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        episode_count += 1\n",
    "\n",
    "        # Update highest reward and episode\n",
    "        if episode_reward > highest_reward:\n",
    "            highest_reward = episode_reward\n",
    "            highest_reward_episode = episode_count\n",
    "    \n",
    "        # Collect rewards for plotting\n",
    "        episode_rewards.append(episode_reward)\n",
    "        running_reward = np.mean(episode_rewards[-100:])\n",
    "        running_rewards.append(running_reward)\n",
    "            \n",
    "        print(\n",
    "            f\"Episode {episode_count} - Reward: {episode_reward:.3f}, \"\n",
    "            f\"Running Reward: {running_reward:.3f}, Epsilon: {epsilon:.3f}, \"\n",
    "            f\"Highest Reward: {highest_reward:.3f} (Episode {highest_reward_episode})\"\n",
    "        )\n",
    "\n",
    "        # Termination condition\n",
    "        if episode_count >= max_episodes:\n",
    "            print(f\"Stopped at episode {episode_count}!\")\n",
    "            # Save the trained model\n",
    "            torch.save(model.state_dict(), \"dqn_trained_model.pth\")\n",
    "            print(\"Model saved to dqn_trained_model.pth\")\n",
    "            break\n",
    "finally:\n",
    "    env.close()\n",
    "\n",
    "# Plot the training performance\n",
    "plot_training_performance(episode_rewards, running_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
